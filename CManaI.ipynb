{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CManaI",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Aglahir/Beer_Tensorflow/blob/master/CManaI.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "57nveqPzOnjO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CMana i"
      ]
    },
    {
      "metadata": {
        "id": "s68R7kCKQgdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Aquí va el proyecto"
      ]
    },
    {
      "metadata": {
        "id": "X2CQr-Y7Nnzg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77d0ed70-cc70-4d5c-a22d-9c84c3e38cd8"
      },
      "cell_type": "code",
      "source": [
        "#El proposito de este proyecto es entrenar a un máquina utilizando machine learning para lograr la identificación de diferentes marcas de cerveza\n",
        "#en un anaquel\n",
        "#MadeWithLove\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "#Limpia \"Basura\" que haya dejado Keras\n",
        "K.clear_session()\n",
        "\n",
        "keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
        "\n",
        "\n",
        "#Habemus GPU  \n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "\n",
        "\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t4dnrCMyLdIB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bienvenidos a CMana i, a trabajar que las máquinas no aprenden solas. "
      ]
    },
    {
      "metadata": {
        "id": "RgB-nMyVQR6k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Tests/Pruebas/Comentarios/Notas/Lo que se les ocurra"
      ]
    },
    {
      "metadata": {
        "id": "14cLk84nP_1N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vv_EtT1NQFkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9eabd2af-c9fa-46ae-d57b-f77ef67eece4"
      },
      "cell_type": "code",
      "source": [
        "print(\"chequen los atajos de teclado para esto, estan arriba en herramientas\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chequen los atajos de teclado para esto, estan arriba en herramientas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jUuznJWdX9OU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "211f8987-768c-4d64-8757-7dac6954a71d"
      },
      "cell_type": "code",
      "source": [
        "print(\"CTRL + M + B   para insertar celda abajo\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CTRL + M + B   para insertar celda abajo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YztsmiAGPbGT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recordatorio amistoso que los capybaras son los roedores más grandes del mundo"
      ]
    },
    {
      "metadata": {
        "id": "fAXAeFbWSfFR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Códigos que rolaron los profes"
      ]
    },
    {
      "metadata": {
        "id": "T9q7bT1YSo91",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TF.Py\n",
        "from __future__ import print_function\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#Parametros de entrenamiento de nuestra red\n",
        "\n",
        "numero_pasos = 500 #El numero de pasos (iteraciones) que se llevaran a cabo en el entrenamiento\n",
        "\n",
        "lotes = 128 #Cada lote de entrenamiento de cuantos elementos va a ser\n",
        "\n",
        "taza_aprendizaje = 0.01 #Que tan rapido aprende el algoritmo \n",
        "\n",
        "display_step = 100 #Cada cuantos pasos de entrenamiento nos desplegara la terminal como va el aprendizaje del programa\n",
        "\n",
        "\n",
        "#Parametros de la estructura de nuestra red\n",
        "\n",
        "numero_entradas = 784 # Numero de entrada que tendremos para nuestra primera capa \n",
        "\n",
        "neuronas_capa1 = 100 # Numero de neuronas en nuestra primera capa\n",
        "\n",
        "neuronas_capa2 = 100 # Numero de neuronas en la segunda capa\n",
        "\n",
        "numero_clasificaciones = 10 # Numero de clasificaciones en nuestra ultima capa\n",
        "\n",
        "\n",
        "\n",
        "#Variables que manejaran las entradas y saliedas de nuestra red\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, numero_entradas])\n",
        "Y = tf.placeholder(\"float\", [None, numero_clasificaciones])\n",
        "\n",
        "#Hacemos dos diccionarios, uno con los pesos que usara nuestra red y el otro con los sesgos o bias\n",
        "diccionario_pesos_W = \\\n",
        "    {\n",
        "    'entrada_capa1': tf.Variable(tf.random_normal([numero_entradas, neuronas_capa1])), #pesos de la entrada a la primera capa\n",
        "    'capa1_capa2': tf.Variable(tf.random_normal([neuronas_capa1, neuronas_capa2])), #pesos de primera a segunda capa \n",
        "    'capa2_salida': tf.Variable(tf.random_normal([neuronas_capa2, numero_clasificaciones])) #pesos de segunda capa a salida\n",
        "    }\n",
        "diccionario_sesgos_b = \\\n",
        "    {\n",
        "    'biascapa1': tf.Variable(tf.random_normal([neuronas_capa1])), #sesgo de primera capa\n",
        "    'biascapa2': tf.Variable(tf.random_normal([neuronas_capa2])), #seso de segunda capa\n",
        "    'biasultimacapa': tf.Variable(tf.random_normal([numero_clasificaciones])) #sesgo de ultima capa \n",
        "    }\n",
        "\n",
        "#Funcion la cual une todo lo que hicimos en los ultimos pasos para ya tener una estructura armada \n",
        "def armar_red_neuronal(x):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    capa1 = tf.add(tf.matmul(x, diccionario_pesos_W['entrada_capa1']), diccionario_sesgos_b['biascapa1'])\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "    capa2 = tf.add(tf.matmul(capa1, diccionario_pesos_W['capa1_capa2']), diccionario_sesgos_b['biascapa2'])\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    capaSalida = tf.matmul(capa2, diccionario_pesos_W['capa2_salida']) + diccionario_sesgos_b['biasultimacapa']\n",
        "    return capaSalida\n",
        "\n",
        "#Construimos la red y hacemos que la ultima capa pase por una funcion de softmax para hacer la clasificacion\n",
        "logits = armar_red_neuronal(X) #Le alimentamos X ya que es la variable que utilizaremos para la primera capa\n",
        "prediccion = tf.nn.softmax(logits) #Cual es la prediccion de nuestra red (Que clasificacion)\n",
        "\n",
        "#Creamos las funciones las cuales se encargan de hacer que nuestro algoritmo aprenda\n",
        "perdida = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizador = tf.train.AdamOptimizer(learning_rate=taza_aprendizaje)\n",
        "minimizador = optimizador.minimize(perdida)\n",
        "\n",
        "#Funciones para medir que tan bien esta aprendiendo el programa durante su entrenamiento\n",
        "predicciones = tf.equal(tf.argmax(prediccion, 1), tf.argmax(Y, 1))\n",
        "certeza = tf.reduce_mean(tf.cast(predicciones, tf.float32))\n",
        "\n",
        "#inicializamos nuestras variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, numero_pasos + 1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(lotes)\n",
        "        \n",
        "        sess.run(minimizador, feed_dict={X: batch_x, Y: batch_y}) #Correr una paso de entrenamiento\n",
        "        \n",
        "        #Todo esto nos enseña que tan bien esta aprendiendo el programa cada 100 pasos de entrenamiento\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            loss, acc = sess.run([perdida, accuracy], feed_dict={X: batch_x,Y: batch_y})\n",
        "            print(\"Paso \"+str(step)+ \",Perdida= \"+\"{:.4f}\".format(loss)+\", Certeza= \"+\"{:.3f}\".format(acc))\n",
        "\n",
        "\n",
        "    print(\"Porcentaje de certeza: {:.2f}\" .format(sess.run(certeza, feed_dict=\n",
        "                                                           {X: mnist.test.images,Y: mnist.test.labels})*100),'%')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9TDtjRMjSivw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CNN.Py\n",
        "import sys\n",
        "import os\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras import optimizers\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "\n",
        "\n",
        "data_entrenamiento = './data/entrenamiento'\n",
        "data_validacion = './data/validacion'\n",
        "\n",
        "\"\"\"\n",
        "Parameters\n",
        "\"\"\"\n",
        "epocas=20\n",
        "longitud, altura = 150, 150\n",
        "batch_size = 32\n",
        "pasos = 1000\n",
        "validation_steps = 300\n",
        "filtrosConv1 = 32\n",
        "filtrosConv2 = 64\n",
        "tamano_filtro1 = (3, 3)\n",
        "tamano_filtro2 = (2, 2)\n",
        "tamano_pool = (2, 2)\n",
        "clases = 3\n",
        "lr = 0.0004\n",
        "\n",
        "\n",
        "##Preparamos nuestras imagenes\n",
        "\n",
        "entrenamiento_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
        "    data_entrenamiento,\n",
        "    target_size=(altura, longitud),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validacion_generador = test_datagen.flow_from_directory(\n",
        "    data_validacion,\n",
        "    target_size=(altura, longitud),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "cnn = Sequential()\n",
        "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(longitud, altura, 3), activation='relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
        "\n",
        "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\"))\n",
        "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(256, activation='relu'))\n",
        "cnn.add(Dropout(0.5))\n",
        "cnn.add(Dense(clases, activation='softmax'))\n",
        "\n",
        "cnn.compile(loss='categorical_crossentropy',\n",
        "            optimizer=optimizers.Adam(lr=lr),\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cnn.fit_generator(\n",
        "    entrenamiento_generador,\n",
        "    steps_per_epoch=pasos,\n",
        "    epochs=epocas,\n",
        "    validation_data=validacion_generador,\n",
        "    validation_steps=validation_steps)\n",
        "\n",
        "target_dir = './modelo/'\n",
        "if not os.path.exists(target_dir):\n",
        "  os.mkdir(target_dir)\n",
        "cnn.save('./modelo/modelo.h5')\n",
        "cnn.save_weights('./modelo/pesos.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}